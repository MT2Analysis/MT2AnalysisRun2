{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.losses import binary_crossentropy\n",
    "from tensorflow.python.keras import optimizers\n",
    "from tensorflow.python.keras import initializers\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "from tensorflow.python import keras\n",
    "import tempfile\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import math \n",
    "import random as rd\n",
    "from sklearn.model_selection import KFold\n",
    "import statistics\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=np.load(\"scripts/significance_output.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=d1.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1400, 1350] bin 33 max 0.44329376642892576 cut 0.725 improve 3.252820495942221 trainfraction [0.5, 0.1]\n",
      "[1400, 1350] bin 19 max 1.8115676338324511 cut 0.7000000000000001 improve 4.262239353084957 trainfraction [0.1, 0.1]\n",
      "[1400, 1350] bin 20 max 1.0635696027373793 cut 0.365 improve 2.016748095851719 trainfraction [0.5, 0.1]\n",
      "[1400, 1350] bin 26 max 0.0 cut 0.0 improve nan trainfraction [0.5, 0.1]\n",
      "[1400, 1350] bin 25 max 2.132218552117699 cut 0.46 improve 2.0431685799799704 trainfraction [0.15, 0.1]\n",
      "[1400, 1350] bin 16 max 1.8858625330847245 cut 0.4 improve 1.3679604144062392 trainfraction [0.25, 0.1]\n",
      "[1400, 1350] bin 27 max 0.0 cut 0.0 improve nan trainfraction [0.5, 0.1]\n",
      "[1400, 1350] bin 34 max 0.0 cut 0.0 improve nan trainfraction [0.5, 0.1]\n",
      "[1400, 1350] bin 18 max 2.1269427494080357 cut 0.77 improve 2.1606006223879306 trainfraction [0.1, 0.1]\n",
      "[1400, 1350] bin 65 max 0.0 cut 0.0 improve nan trainfraction [0.5, 0.1]\n",
      "[1400, 1350] bin 23 max 0.0 cut 0.0 improve nan trainfraction [0.5, 0.15]\n",
      "[1400, 1350] bin 32 max 0.0 cut 0.0 improve nan trainfraction [0.1, 0.1]\n",
      "[1400, 1350] bin 66 max 0.0 cut 0.0 improve nan trainfraction [0.5, 0.1]\n",
      "[1400, 1350] bin 55 max 0.0 cut 0.0 improve nan trainfraction [0.5, 0.1]\n",
      "[1400, 1350] bin 15 max 2.6742991684060256 cut 0.6 improve 1.9557015418616743 trainfraction [0.1, 0.1]\n",
      "[1400, 1350] bin 45 max 0.07489679971656679 cut 0.515 improve 3.14627628289047 trainfraction [0.25, 0.1]\n",
      "[1400, 1350] bin 46 max 1.1826951708402813 cut 0.5650000000000001 improve 2.7687907988250258 trainfraction [0.5, 0.1]\n",
      "[1400, 1350] bin 54 max 0.0 cut 0.0 improve nan trainfraction [0.5, 0.15]\n",
      "[1400, 1350] bin 84 max 0.36948912117021926 cut 0.485 improve 1.0678036272842575 trainfraction [0.5, 0.5]\n",
      "[2000, 1600] bin 20 max 4.762848578315061 cut 0.5 improve 1.8967530718239374 trainfraction [0.5, 0.1]\n",
      "[2000, 1600] bin 47 max 0.973881070545727 cut 0.375 improve 1.1487008678613089 trainfraction [0.5, 0.1]\n",
      "[2000, 1600] bin 13 max 1.947252607155262 cut 0.85 improve 3.0108577581758422 trainfraction [0.5, 0.1]\n",
      "[2000, 1600] bin 46 max 0.5541391170743902 cut 0.66 improve 1.2844229659366015 trainfraction [0.5, 0.1]\n",
      "[2000, 1600] bin 27 max 2.515116030835066 cut 0.885 improve 3.2166391861096497 trainfraction [0.5, 0.1]\n",
      "[2000, 1600] bin 57 max 0.0 cut 0.0 improve nan trainfraction [0.5, 0.1]\n",
      "[2000, 1600] bin 34 max 0.14947742745677212 cut 0.125 improve 1.6124830249416757 trainfraction [0.5, 0.1]\n",
      "[2000, 1600] bin 66 max 0.10846747886817783 cut 0.58 improve 14.721390703280585 trainfraction [0.5, 0.1]\n",
      "[2400, 1400] bin 34 max 2.2872457470630376 cut 0.935 improve 1.6955444390953867 trainfraction [0.5, 0.1]\n",
      "[2400, 1400] bin 27 max 1.2074762541558335 cut 0.76 improve 3.3086613681458994 trainfraction [0.5, 0.1]\n",
      "[2400, 1400] bin 33 max 2.412396704270158 cut 0.97 improve 4.616302381445044 trainfraction [0.5, 0.1]\n",
      "[2400, 1400] bin 66 max 1.4730449778954786 cut 0.58 improve 2.9271047069198834 trainfraction [0.5, 0.1]\n",
      "[2400, 1400] bin 67 max 0.3764765158369462 cut 0.68 improve 2.652181931862449 trainfraction [0.5, 0.1]\n",
      "[2400, 1400] bin 65 max 0.7195078071188363 cut 0.9500000000000001 improve 4.262149231595641 trainfraction [0.5, 0.1]\n",
      "[2400, 200] bin 33 max 1.8176537567755264 cut 0.97 improve 2.748068106762974 trainfraction [0.5, 0.1]\n",
      "[2400, 200] bin 34 max 1.4240559192675155 cut 0.5 improve 1.8018907704468154 trainfraction [0.5, 0.1]\n",
      "[2400, 200] bin 65 max 1.0483763785594669 cut 0.91 improve 3.77990915765824 trainfraction [0.5, 0.1]\n",
      "[2400, 200] bin 32 max 1.2158421664295245 cut 0.9400000000000001 improve 4.103996777154659 trainfraction [0.1, 0.1]\n",
      "[2400, 200] bin 66 max 1.0242057736841408 cut 0.58 improve 3.0844717301786284 trainfraction [0.5, 0.1]\n",
      "[2400, 200] bin 64 max 0.5029021935732596 cut 0.9450000000000001 improve 3.491679387657627 trainfraction [0.25, 0.1]\n",
      "[2400, 200] bin 67 max 0.4306101366465552 cut 0.31 improve 1.8953755786311044 trainfraction [0.5, 0.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/wjin/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "for mass in range(4):\n",
    "    for binindex in range(len(dataset['binindex'][mass])):\n",
    "        sig=dataset['significance'][mass][binindex]\n",
    "        print(dataset['mass'][mass],'bin',dataset['binindex'][mass][binindex], 'max', np.max(sig), 'cut',0.005*sig.index(max(sig)),\n",
    "             'improve',max(sig)/sig[0], 'trainfraction',dataset['trainfraction'][mass][binindex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.36494404225853927,\n",
       " 0.36494404225853927,\n",
       " 0.36494404225853927,\n",
       " 0.36494404225853927,\n",
       " 0.36494404225853927,\n",
       " 0.36494404225853927,\n",
       " 0.36494404225853927,\n",
       " 0.36494404225853927,\n",
       " 0.3792536439402507,\n",
       " 0.39042033418139244,\n",
       " 0.40748509345242934,\n",
       " 0.42043530172587185,\n",
       " 0.4210860381917415,\n",
       " 0.421366179229482,\n",
       " 0.42522322142892227,\n",
       " 0.4439525264282004,\n",
       " 0.4676604532400309,\n",
       " 0.4676604532400309,\n",
       " 0.49053942148834995,\n",
       " 0.5177869847016254,\n",
       " 0.5182706180844131,\n",
       " 0.5250252439398548,\n",
       " 0.5400668659756521,\n",
       " 0.5404143198283523,\n",
       " 0.5404143198283523,\n",
       " 0.5404143198283523,\n",
       " 0.5526111873091272,\n",
       " 0.5536442616296969,\n",
       " 0.5536442616296969,\n",
       " 0.5539744252223365,\n",
       " 0.5539744252223365,\n",
       " 0.5539744252223365,\n",
       " 0.589569596171264,\n",
       " 0.5903729127292151,\n",
       " 0.5903729127292151,\n",
       " 0.5903729127292151,\n",
       " 0.5930075519581658,\n",
       " 0.5930075519581658,\n",
       " 0.5930075519581658,\n",
       " 0.6020690092716219,\n",
       " 0.6020690092716219,\n",
       " 0.6029046393868704,\n",
       " 0.6029046393868704,\n",
       " 0.6083792605443774,\n",
       " 0.6083792605443774,\n",
       " 0.6754482950353988,\n",
       " 0.6754482950353988,\n",
       " 0.6754482950353988,\n",
       " 0.6770577562110833,\n",
       " 0.6770577562110833,\n",
       " 0.6770577562110833,\n",
       " 0.6770577562110833,\n",
       " 0.6770577562110833,\n",
       " 0.6770577562110833,\n",
       " 0.6770577562110833,\n",
       " 0.7564336524701529,\n",
       " 0.7591724861021196,\n",
       " 0.7592529991621437,\n",
       " 0.8559465042381788,\n",
       " 0.8559465042381788,\n",
       " 0.8589542583203132,\n",
       " 0.8589542583203132,\n",
       " 0.891752274259277,\n",
       " 0.9147239982336685,\n",
       " 0.9147239982336685,\n",
       " 0.9147239982336685,\n",
       " 0.9147239982336685,\n",
       " 0.9147239982336685,\n",
       " 0.9147239982336685,\n",
       " 0.9147239982336685,\n",
       " 0.9174015466428387,\n",
       " 0.9214520650627307,\n",
       " 0.9214520650627307,\n",
       " 0.9214520650627307,\n",
       " 0.9214520650627307,\n",
       " 0.9214520650627307,\n",
       " 0.9214520650627307,\n",
       " 0.9214520650627307,\n",
       " 0.9214520650627307,\n",
       " 0.9214520650627307,\n",
       " 0.9214520650627307,\n",
       " 0.9214520650627307,\n",
       " 0.9294433109693584,\n",
       " 0.9294433109693584,\n",
       " 0.9294433109693584,\n",
       " 0.9294433109693584,\n",
       " 0.9406654698303154,\n",
       " 0.9406654698303154,\n",
       " 0.9560815727578824,\n",
       " 1.0049478723095233,\n",
       " 1.0049478723095233,\n",
       " 1.0049478723095233,\n",
       " 1.0049478723095233,\n",
       " 1.0049478723095233,\n",
       " 1.0049478723095233,\n",
       " 1.0049478723095233,\n",
       " 1.0049478723095233,\n",
       " 1.0049478723095233,\n",
       " 1.0065571483808724,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.0091834069660521,\n",
       " 1.1996295045383534,\n",
       " 1.1996295045383534,\n",
       " 1.1996295045383534,\n",
       " 1.1996295045383534,\n",
       " 1.2074762541558335,\n",
       " 1.2074762541558335,\n",
       " 1.2074762541558335,\n",
       " 1.2074762541558335,\n",
       " 1.2074762541558335,\n",
       " 1.2074762541558335,\n",
       " 1.2074762541558335,\n",
       " 1.2074762541558335,\n",
       " 1.2074762541558335,\n",
       " 1.2074762541558335,\n",
       " 1.0233281024182788,\n",
       " 1.0233281024182788,\n",
       " 1.0233281024182788,\n",
       " 0.9865388054948255,\n",
       " 0.9865388054948255,\n",
       " 0.9865388054948255,\n",
       " 0.9865388054948255,\n",
       " 0.9865388054948255,\n",
       " 0.9865388054948255,\n",
       " 0.9865388054948255,\n",
       " 0.9398010699108215,\n",
       " 0.9398010699108215,\n",
       " 1.071144821434755,\n",
       " 0.9233019106569357,\n",
       " 0.9679075900596548,\n",
       " 0.9238854631850864,\n",
       " 0.8903540926325774,\n",
       " 0.8903540926325774,\n",
       " 0.8487927186544019,\n",
       " 0.9022337857061867,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['significance'][2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[20,0.5],[47,0.375],[13,0.85],[46,0.66],[27,0.885],[34,0.935],[66,0.58],[33,0.97],\n",
    " [67,0.68],[65,0.91],[32,0.94],[64,0.945],[33,0.725],[19,0.70],[20,0.365],[25,0.46],]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
